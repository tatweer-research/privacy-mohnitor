# Dataset

The pretraining is done with the [PrivaSeer](https://privaseer.ist.psu.edu/) dataset proposed by [Srinath et al.](https://arxiv.org/abs/2004.11131).

### Training ###
`python3 run_t5_mlm_flax.py --output_dir="./privaseer" --model_name_or_path="t5-small" --model_type="t5-small" --config_name="t5-small" --tokenizer_name="t5-small" --dataset_name="alzoubi36/privaseer" --dataset_config_name="alzoubi36/privaseer" --max_seq_length="512" --per_device_train_batch_size="32" --per_device_eval_batch_size="32" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10000" --eval_steps="2500" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer/log.txt`

`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test_flax/privaseer/" --model_name_or_path="t5-base" --model_type="t5-base" --config_name="t5-base" --tokenizer_name="t5-base" --dataset_name="alzoubi36/privaseer" --dataset_config_name="alzoubi36/privaseer" --max_seq_length="512" --per_device_train_batch_size="16" --per_device_eval_batch_size="16" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10000" --eval_steps="2500" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test_flax/privaseer/log.txt`


`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-large/" --model_name_or_path="t5-large" --model_type="t5-large" --config_name="t5-large" --tokenizer_name="t5-large" --dataset_name="alzoubi36/privaseer" --dataset_config_name="alzoubi36/privaseer" --max_seq_length="512" --per_device_train_batch_size="8" --per_device_eval_batch_size="8" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10000" --eval_steps="2500" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-large/log.txt`


`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test-flax/privaseer-3b/" --model_name_or_path="google/t5-v1_1-xl" --model_type="google/t5-v1_1-xl" --config_name="google/t5-v1_1-xl" --tokenizer_name="google/t5-v1_1-xl" --dataset_name="alzoubi36/privaseer" --dataset_config_name="alzoubi36/privaseer" --max_seq_length="512" --per_device_train_batch_size="8" --per_device_eval_batch_size="8" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="2000" --eval_steps="2000" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer-3b/log.txt`


### Demo Training ###
`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test-flax/privaseer_demo/" --model_name_or_path="t5-small" --model_type="t5-small" --config_name="t5-small" --tokenizer_name="t5-small" --dataset_name="alzoubi36/privaseer_demo" --dataset_config_name="alzoubi36/privaseer_demo" --max_seq_length="512" --per_device_train_batch_size="32" --per_device_eval_batch_size="32" --adafactor --learning_rate="0.005" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10" --eval_steps="10" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer_demo/log.txt`

`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test-flax/privaseer_demo/" --model_name_or_path="t5-base" --model_type="t5-base" --config_name="t5-base" --tokenizer_name="t5-base" --dataset_name="alzoubi36/privaseer_demo" --dataset_config_name="alzoubi36/privaseer_demo" --max_seq_length="512" --per_device_train_batch_size="16" --per_device_eval_batch_size="16" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10" --eval_steps="10" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer_demo/log.txt`

`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-large/" --model_name_or_path="t5-large" --model_type="t5-large" --config_name="t5-large" --tokenizer_name="t5-large" --dataset_name="alzoubi36/privaseer_demo" --dataset_config_name="alzoubi36/privaseer_demo" --max_seq_length="512" --per_device_train_batch_size="8" --per_device_eval_batch_size="8" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10" --eval_steps="10" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-large/log.txt`

`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-3b/" --model_name_or_path="t5-3b" --model_type="t5-3b" --config_name="t5-3b" --tokenizer_name="t5-3b" --dataset_name="alzoubi36/privaseer_demo" --dataset_config_name="alzoubi36/privaseer_demo" --max_seq_length="512" --per_device_train_batch_size="1" --per_device_eval_batch_size="1" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10" --eval_steps="10" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-3b/log.txt`

`python3 run_t5_mlm_flax.py --output_dir="/home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-11b/" --model_name_or_path="t5-3b" --model_type="t5-3b" --config_name="t5-3b" --tokenizer_name="t5-3b" --dataset_name="alzoubi36/privaseer_demo" --dataset_config_name="alzoubi36/privaseer_demo" --max_seq_length="512" --per_device_train_batch_size="1" --per_device_eval_batch_size="1" --adafactor --learning_rate="0.001" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="10" --eval_steps="10" --num_train_epochs="1" 2>&1 | tee /home/Mohammad.Al-Zoubi/test-flax/privaseer-t5-11b/log.txt`
